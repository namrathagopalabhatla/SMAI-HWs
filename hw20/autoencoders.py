# -*- coding: utf-8 -*-
"""Autoencoders.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QvuE1lGChpC3epqB7L6iOJKzjJ--peqz

# Autoencoders

### A feed forward neural network which is trained to copy its input to output !

![alt text](images/Autoencoder.png "Title")

<h2>Encoder: $h=f(x)$ <br>
Decoder: $r=g(h)$ <br>
Loss: $L(x,g(f(x)))$
</h2>

<h3>Q. Does g(f(x)) == x useful ??</h3>

AE's are designed to be unable to learn to copy perfectly, thereby limiting to extract useful properties alone.

### Need for AE's 
- Feature Learning
- Dimensionality Reduction

### Variants of AE's
- Undercomplete
- Regularized
    - Sparse: $L(x,g(f(x)))+\Omega(h)$
    - Denoising: $L(x,g(f(\tilde{x})))$
    - Penalizing Derivatives: $L(x,g(f(x)))+\Omega(h,x)$ where $\Omega(h,x)=\lambda\sum_{i}||\nabla_x h_i ||^2$

## Simple Model -- Undercomplete AE
"""

# Code here
import torch
import torch.nn as nn
import torchvision
import torchvision.datasets as datasets
import torchvision.transforms as transforms
import torch.nn.init as weight_init
import matplotlib.pyplot as plt
import pdb


#parameters
batch_size = 128

preprocess = transforms.Compose([
                           transforms.ToTensor(),
                           transforms.Normalize((0.1307,), (0.3081,))
                       ])

#Loading the train set file
dataset = datasets.MNIST(root='./data',
                            transform=preprocess,  
                            download=True)

loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=True)

"""### Autoencoder Class"""

class AE(nn.Module):
    def __init__(self,layer1,layer2):
        super().__init__()
        self.encoder = nn.Sequential(
            nn.Linear(28*28, layer1),
            nn.ReLU(),
            nn.Linear(layer1,layer2),
            nn.ReLU(),
            nn.Linear(layer2,2),
        )
        self.decoder = nn.Sequential(
            nn.Linear(2, layer2),
            nn.ReLU(),
            nn.Linear(layer2, layer1),
            nn.ReLU(),
            nn.Linear(layer1, 28*28),
            nn.Tanh()
        )
    
    def forward(self,x):
        h = self.encoder(x)
        xr = self.decoder(h)
        return xr,h

#Misc functions
def loss_plot(losses,hidden_neurons):
    max_epochs = len(losses)
    times = list(range(1, max_epochs+1))
    plt.figure(figsize=(30, 7))
    plt.title("Optimizer is RMSProp")
    plt.xlabel("epochs")
    plt.ylabel("cross-entropy loss")
    return plt.plot(hidden_neurons, losses)

# use_cuda = torch.cuda.is_available()
# device = torch.device("cuda" if use_cuda else "cpu")
# print('Using CUDA ', use_cuda)

# net = AE()
# net = net.to(device)

# #Mean square loss function
# criterion = nn.MSELoss()

# #Parameters
# learning_rate = 1e-2
# weight_decay = 1e-5

# #Optimizer and Scheduler
# optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=0.001, patience=5, verbose = True)

#optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=momentum_rate)

##RMSProp
# torch.optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
##Adam
# torch.optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)
##Adagrad
# torch.optim.Adagrad(params, lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)

# Commented out IPython magic to ensure Python compatibility.
num_epochs = 5

#Training
epochLoss = []
layer_1=[392,196,98,49,28]
layer_2=[49,16,14,7,7]
hidden_neurons=[]
k=0
while k<5:
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")
    print('Using CUDA ', use_cuda)

    net = AE(layer_1[k],layer_2[k])
    net = net.to(device)
    hidden_neurons.append((layer_1[k]+layer_2[k]+2)*2)
    #Mean square loss function
    criterion = nn.MSELoss()

    #Parameters
    learning_rate = 1e-2
    weight_decay = 1e-5

    #Optimizer and Scheduler
    #optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
    optimizer=torch.optim.RMSprop(net.parameters(), lr=learning_rate, weight_decay=weight_decay,centered=False)
    #optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.0)
    #optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)
    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, threshold=0.001, patience=5, verbose = True)
    for epoch in range(num_epochs):
        total_loss, cntr = 0, 0
        for i,(images,_) in enumerate(loader):
            
            images = images.view(-1, 28*28)
            images = images.to(device)
            
            # Initialize gradients to 0
            optimizer.zero_grad()
            
            # Forward pass (this calls the "forward" function within Net)
            outputs, _ = net(images)
            
            # Find the loss
            loss = criterion(outputs, images)
            
            # Find the gradients of all weights using the loss
            loss.backward()
            
            # Update the weights using the optimizer and scheduler
            optimizer.step()
          
            total_loss += loss.item()
            cntr += 1
        
        scheduler.step(total_loss/cntr)
        print ('Epoch [%d/%d], Loss: %.4f' 
#                       %(epoch+1, num_epochs, total_loss/cntr))
    epochLoss.append(total_loss/cntr)
    print('Hidden-Neurons %d' %(hidden_neurons[k]))
    k=k+1

_ = loss_plot(epochLoss,hidden_neurons)

len(dataset)

"""### Save Model"""

net = net.to("cpu")
torch.save(net.state_dict(),'ae_model.ckpt')

"""### Load Model"""

#Load model
net = AE()
checkpoint = torch.load('ae_model.ckpt')
net.load_state_dict(checkpoint)
net = net.to(device)

"""### Feature Extraction"""

#Feature Extraction
ndata = len(dataset)
hSize = 2

test_dataset = datasets.MNIST(root='./data',
                            transform=preprocess,  
                            download=True)
test_loader = torch.utils.data.DataLoader(dataset=dataset, batch_size=batch_size, shuffle=False)

iMat = torch.zeros((ndata,28*28))
rMat = torch.zeros((ndata,28*28))
featMat = torch.zeros((ndata,hSize))
labelMat = torch.zeros((ndata))
cntr=0

with torch.no_grad():
    for i,(images,labels) in enumerate(loader):

        images = images.view(-1, 28*28)
        images = images.to(device)
        
        rImg, hFeats = net(images)
        
        iMat[cntr:cntr+batch_size,:] = images
        rMat[cntr:cntr+batch_size,:] = (rImg+0.1307)*0.3081
        
        featMat[cntr:cntr+batch_size,:] = hFeats
        labelMat[cntr:cntr+batch_size] = labels
        
        cntr+=batch_size
        
        if cntr>=ndata:
        	break

"""### Reconstruction"""

#Reconstruction
plt.figure()
plt.axis('off')
plt.imshow(rMat[1,:].view(28,28),cmap='gray')

plt.figure(figsize = (15,5))
# plt.subplot(131)
plt.scatter(featMat[:,0],featMat[:,1],  c = labelMat)
plt.title('AE Scatter Plot')

"""## PCA vs Autoencoders"""

from sklearn.decomposition import PCA

iNPMat = iMat.numpy()

pca = PCA(n_components=2)
pca.fit(iNPMat)

#Projection
pcaProj = (iNPMat - pca.mean_).dot(pca.components_.T)
print(pcaProj.shape)

#Reconstruction
pcaRec = pcaProj.dot(pca.components_) + pca.mean_
print(pcaRec.shape)

#Reconstruction
plt.figure()
plt.axis('off')
plt.imshow(pcaRec[100,:].reshape(28,28),cmap='gray')

plt.figure(figsize = (15,5))
# plt.subplot(131)
plt.scatter(pcaProj[:,0],pcaProj[:,1],  c = labelMat)
plt.title('PCA Scatter Plot')

